{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmasher as cmr\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "colors = cmr.take_cmap_colors('cmr.bubblegum', 6, cmap_range=(0.05, 0.95), return_fmt='hex')\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['mathtext.default'] = 'regular'\n",
    "from matplotlib import colors as cr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from astropy.cosmology import Planck18_arXiv_v2 as Planck18\n",
    "from astropy import units as u\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import catgen\n",
    "import sfh\n",
    "import param_dists as pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some helpful information for the Catalog class we are going to specify. \n",
    "####  - n_sample is the number of draws in metallicity and redshift we will take from the cosmic star formation history\n",
    "####  - n_downsample is the factor by which we downsample the draws\n",
    "####  - the path and label strings help with reading in data\n",
    "####  - mets is our metallicity grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100000\n",
    "n_downsample=1\n",
    "path = '/Users/kbreivik/projects/emulator_models/alpha_1/'\n",
    "label = r'$\\alpha=1$'\n",
    "mets = np.logspace(np.log10(0.000085), np.log10(0.034), 16)\n",
    "mets = mets[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we initialize our catalog class with our chosen star formation history model (btwn Madau & Dickinson 2014 or Madau & Fragos 2017), the metallicity grid, and the kstar types and SF data for reading in the COSMIC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbreivik/Documents/Github/cosmic-mergers/utils.py:254: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  dat = np.array(dat)\n"
     ]
    }
   ],
   "source": [
    "cat = catgen.Catalog(dat_path=path,                     \n",
    "                     sfh_model=sfh.mf_17,\n",
    "                     met_grid=mets, \n",
    "                     kstar_1='13_14', \n",
    "                     kstar_2='13_14', \n",
    "                     SFstart=13700.0, \n",
    "                     SFduration=0.0,\n",
    "                     pessimistic_cut=True,\n",
    "                     kstar_1_select=[13],\n",
    "                     kstar_2_select=[14])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how the grid looks with the merger formation efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0,\n",
       "       13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.merger_dat[0][:,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbreivik/Documents/Github/cosmic-mergers/utils.py:254: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  dat = np.array(dat)\n"
     ]
    }
   ],
   "source": [
    "cat2 = catgen.Catalog(dat_path=path,                     \n",
    "                      sfh_model=sfh.mf_17,\n",
    "                      met_grid=mets, \n",
    "                      kstar_1='13_14', \n",
    "                      kstar_2='13_14', \n",
    "                      SFstart=13700.0, \n",
    "                      SFduration=0.0,\n",
    "                      pessimistic_cut=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.0, 13.0, 13.0, ..., 14.0, 14.0, 14.0], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2.merger_dat[0][:,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log10(mets/0.014), cat.n_merger/cat.M_sim, drawstyle='steps-mid')\n",
    "plt.plot(np.log10(mets/0.014), cat.n_merger/cat.M_sim)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('metallicity')\n",
    "plt.ylabel('BBH formation efficiency [M$_{\\odot}^{-1}$]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even though the number of sources per grid varies, since we normalize by the total amount of mass drawn from the IMF/mass ratio distribution, the formation efficiency changes smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log10(mets/0.014), cat.n_BBH, drawstyle='steps-mid')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('metallicity')\n",
    "plt.ylabel('N per metallicity grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mets, cat.n_BBH, drawstyle='steps-mid')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a catalog which contains mergers in a comoving volume of [Gpc$^3$]\n",
    "\n",
    "The catalog building process draws `n_sample` metallicities and redshifts which specify ZAMS formation metallicities and redshifts to our BBH mergers. Since the MD14 and MD17 star formation rates are reported as $\\frac{dM}{dtdV}(z)$, which is the total amount of mass formed per year per comoving Gpc$^3$ and we would like to sample $\\frac{dM}{dzdV}(z)$, we need to transform the star formation rate as $\\frac{dM}{dzdV}(z) = \\frac{dM}{dtdV}(z) \\frac{dt}{dz}(z)$. Now, all of the BBH mergers in the catalog we generate will be for a comoving volume of $1\\rm{Gpc}^3$.\n",
    "\n",
    "Note that sigma_logZ is the standard deviation of the metallicity distribution, where the mean is specified by Madau and Fragos 2017.  Also note that both sigma_logZ and z_max are not necessary to specify if you want to use the default values which are sigma_logZ=0.5 and z_max=20.\n",
    "\n",
    "Once we sample a bunch of ZAMS formation redshifts and metallicities, we can attach each formation redshift and metallicity to a BBH merger. We then log the formation lookback time, and the merger lookback time. To make the normalization of the rate kdes easy in future calculations, we don't filter out systems with merger lookback times that will occur in the future (i.e. $t_{\\rm{merge}} < 0$). During this sampling process, we also keep track of the metallicity grid point that each BBH is attached to so that we can normalize the mass formed in the star formation history to the average amount of mass formed per BBH merger at a given metallicity, which we define as $M_{\\rm{BBH,mean}}(Z)$.\n",
    "\n",
    "After the sampling process, we need to figure out how to normalize our catalog of BBH mergers in a comoving volume to a Universe of stars formed in a comoving volume. The total amount of stars formed in a comoving volume is just $M_{\\rm{\\star,com}} = \\frac{dM}{dV} = \\int_{0}^{t_{\\rm{lb}}(z_{\\rm{max}})} dt_{\\rm{lb}} \\frac{dM}{dtdV}(z(t_{\\rm{lb}}))$. Then the conversion to the number of BBH mergers per comoving volume at any lookback time (or redshift) can be calculated by using the normalization factor: $\\frac{M_{\\rm{\\star,com}}}{M_{\\rm{BBH,mean}}}$, where $M_{\\rm{BBH,mean}}$ is the average amount of mass formed per BBH merger at metallicity $Z$.\n",
    "\n",
    "Now that we have a catalog of mergers including their formation redshifts, formation lookback times, and merger lookback times, as well as the normalization factor to scale our catalog to the Universe, we are ready to look at the merger distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers, norm_fac = cat.build_cat(n_sample=n_sample, n_downsample=n_downsample, sigma_logZ=0.5, z_max=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pdf of the merger lookback time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tlb = pdf.get_dN_dtlb_dV(np.array(mergers.t_merge, dtype=float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate a set of lookback times and redshifts to evaulate the pdf\n",
    "Note that everything is in units of a comoving volume already so no conversion is necessary to make this kde! Also, since the units of a pdf of $t_{\\rm{lb}}$ are [yr$^{-1}$] and our catalog is in units of [Gpc$^{-3}$], the pdf we created has units of [yr$^{-1}$Gpc$^{-3}$] and the rate per comoving volume is just the pdf multiplied by the normalization factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_max = 15\n",
    "z = np.expm1(np.linspace(0, np.log1p(z_max), 150))\n",
    "tlb = Planck18.lookback_time(z).to(u.Myr).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tlb, sfh.mf_17(z)*100, label='MF17 * 100')\n",
    "plt.plot(tlb, p_tlb(tlb)*norm_fac, label='p(tlb)')\n",
    "plt.legend()\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel(r'dN/(dt$_{\\rm{lb}}$dV)(z) [Myr$^{-1}$ Gpc$^{-3}$]')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tlb, p_tlb(tlb), label='p(tlb)')\n",
    "plt.hist(mergers.t_merge, bins=200, histtype='step', density=True)\n",
    "plt.plot(tlb, sfh.mf_17(z)/1000, label='MF17 / 1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(mergers.loc[(mergers.z_form < 1.1) & (mergers.z_form >= 0.9)].met), \n",
    "         bins=15, color='black', histtype='step', density=True, label='z=1')\n",
    "plt.axvline(sfh.mean_metal_log_z(1.0), ls='--', lw=3, c='black')\n",
    "\n",
    "#plt.hist(np.log(mergers.loc[(mergers.z_form < 2.1) & (mergers.z_form >= 1.9)].met), \n",
    "#         bins=20, color='silver', histtype='step', density=True)\n",
    "#plt.axvline(sfh.mean_metal_log_z(2.0), ls='--', lw=3, c='silver')\n",
    "\n",
    "#plt.hist(np.log(mergers.loc[(mergers.z_form < 3.1) & (mergers.z_form >= 2.9)].met), \n",
    "#         bins=20, color='teal', histtype='step', density=True)\n",
    "#plt.axvline(sfh.mean_metal_log_z(3.0), ls='--', lw=3, c='teal')\n",
    "\n",
    "plt.hist(np.log(mergers.loc[(mergers.z_form < 5.1) & (mergers.z_form >= 4.9)].met), \n",
    "         bins=15, color='orange', histtype='step', density=True, label='z=5')\n",
    "plt.axvline(sfh.mean_metal_log_z(5.0), ls='--', lw=3, c='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z, sfh.mean_metal_log_z(z, Zsun=0.017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinterp = interp1d(tlb, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(zinterp(mergers.loc[mergers.t_merge > 0].t_merge), bins=20, density=True)\n",
    "#plt.plot(z, p_tlb(tlb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z, p_tlb(tlb) * norm_fac, label='rate')\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel(r'R$_{\\rm{com}}$(z) [yr$^{-1}$ Gpc$^{-3}$]')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can zoom into the nearby universe to get the rate that LIGO can observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z, p_tlb(tlb) * norm_fac)\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel(r'R$_{\\rm{com}}$(z) [yr$^{-1}$ Gpc$^{-3}$]')\n",
    "plt.xlim(0, 1)\n",
    "#plt.ylim(0, 20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(np.log10(mergers.met/0.017), np.log10((mergers.t_form - mergers.t_merge)), bins=50, norm=cr.LogNorm());\n",
    "plt.xlabel('log10(Z/Zsun)')\n",
    "plt.ylabel('log10(delay time/Myr)')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.savefig('qcflag_4_old.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also get the pdf of merger redshifts in the universe as measured from the detector frame\n",
    "\n",
    "In this case, since we are moving out of the comoving detector frame, we will need a Jacobian to convert our comoving volume catalog to the full universe volume *and* a Jacobian to convert the merger redshift in the source frame to the detector frame. The process is then to build a kde of the merger lookback times which gives us $\\frac{dN}{dtdV}$ and multiply by $\\frac{dV}{dz}$ and $\\frac{dt_{\\rm{source}}}{dt_{\\rm{detector}}} = \\frac{1}{1 + z_{\\rm{merge}}}$, this gives us $\\frac{dN}{dt_{\\rm{det}}dz} = \\frac{dN}{dt_{\\rm{source}}dV}\\frac{dV}{dz}\\frac{dt_{\\rm{source}}}{dt_{\\rm{detector}}}$. Now if we want to properly normalize the pdf $\\frac{dN}{dt_{\\rm{det}}dz}$, we integrate over the lookback times as $\\rm{norm} = \\int dt_{\\rm{det}} \\frac{dN}{dt_{\\rm{det}}dz}$, then $p(z,t) = \\frac{1}{\\rm{norm}}{\\frac{dN}{dt_{\\rm{det}}dz}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pz = pdf.get_pz(np.array(mergers.t_merge, dtype=float), z_max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z, pz(z))\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel(r'p(z)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And if we want to get the rate per mass per q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.array(mergers.m1, dtype=float)\n",
    "m2 = np.array(mergers.m2, dtype=float)\n",
    "q = m2/m1\n",
    "t = np.array(mergers.t_merge, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_mass = np.where(m1 > m2, m1, m2)\n",
    "mass_ratio = np.where(m1 > m2, m2/m1, m1/m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dN_dtlb_dlnm_dV = pdf.get_dN_dtlb_dlnm_dV(t_lb=t, m=primary_mass)\n",
    "dN_dtlb_dlnm_dlogitq_dV = pdf.get_dN_dtlb_dlnm_dq_dV(t_lb=t, m=primary_mass, q=mass_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0.1, 50, len(tlb)), dN_dtlb_dlnm_dV(Planck18.lookback_time(0.01).to(u.Myr), \n",
    "                                                       np.linspace(0.1, 50, len(tlb))) * norm_fac, \n",
    "         label='z=0.01')\n",
    "plt.plot(np.linspace(0.1, 50, len(tlb)), dN_dtlb_dlnm_dV(Planck18.lookback_time(0.1).to(u.Myr), \n",
    "                                                       np.linspace(0.1, 50, len(tlb))) * norm_fac, \n",
    "         label='z=0.1')\n",
    "plt.plot(np.linspace(0.1, 50, len(tlb)), dN_dtlb_dlnm_dV(Planck18.lookback_time(1).to(u.Myr), \n",
    "                                                       np.linspace(0.1, 50, len(tlb))) * norm_fac, \n",
    "         label='z=1')\n",
    "plt.plot(np.linspace(0.1, 50, len(tlb)), dN_dtlb_dlnm_dV(Planck18.lookback_time(2).to(u.Myr), \n",
    "                                                       np.linspace(0.1, 50, len(tlb))) * norm_fac, \n",
    "         label='z=2')\n",
    "\n",
    "plt.ylabel('dN/dtdlnMdV(z) [Gpc$^{-3}$ yr$^{-1}$ lnM$^{-1}$]')\n",
    "plt.xlabel('Mass [Msun]')\n",
    "plt.legend()\n",
    "plt.ylim(0.01, 150)\n",
    "plt.xlim(8, 50)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0.1, 50, len(tlb)), dN_dtlb_dlnm_dlogitq_dV(Planck18.lookback_time(1).to(u.Myr), \n",
    "                                                                 np.linspace(0.1, 50, len(tlb)),\n",
    "                                                                 0.9) * norm_fac, \n",
    "         label='z=1, q=0.9')\n",
    "\n",
    "plt.plot(np.linspace(0.1, 50, len(tlb)), dN_dtlb_dlnm_dlogitq_dV(Planck18.lookback_time(1).to(u.Myr), \n",
    "                                                                 np.linspace(0.1, 50, len(tlb)),\n",
    "                                                                 0.5) * norm_fac, \n",
    "         label='z=1, q=0.5')\n",
    "\n",
    "plt.plot(np.linspace(0.1, 50, len(tlb)), dN_dtlb_dlnm_dlogitq_dV(Planck18.lookback_time(1).to(u.Myr), \n",
    "                                                                 np.linspace(0.1, 50, len(tlb)),\n",
    "                                                                 0.1) * norm_fac, \n",
    "         label='z=1, q=0.1')\n",
    "\n",
    "plt.ylabel('dN/dtdlnMdlogitqdV(z) [Gpc$^{-3}$ yr$^{-1}$ lnM$^{-1} logitq^{-1}$]')\n",
    "plt.xlabel('Mass [Msun]')\n",
    "plt.legend()\n",
    "plt.ylim(1e-3, 100)\n",
    "plt.xlim(8, 50)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(primary_mass, histtype='step', label='primary mass', bins=40, lw=3, color='black')\n",
    "plt.hist(mergers.m1, histtype='step', label='first formed', bins=40, lw=1.5, color='teal')\n",
    "plt.hist(mergers.m2, histtype='step', label='second formed', bins=40, lw=1.5, color='orchid')\n",
    "plt.xlabel(r'Mass [M$_{\\odot}$]')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(mergers.m1, mergers.m2, bins=100, norm=cr.LogNorm())\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'first formed mass [M$_{\\odot}$]')\n",
    "plt.ylabel(r'second formed mass [M$_{\\odot}$]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.log(mergers.t_form/1000) - np.log(max(mergers.t_form/1000 + 0.1) - mergers.t_form/1000)\n",
    "Y = np.log(mergers.met/0.014) - np.log(max(mergers.met/0.014 + 0.001) - mergers.met/0.014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[np.isinf(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(X, Y, bins=100, norm=cr.LogNorm());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(mergers.t_form/1000, np.log10(mergers.met/0.017), bins=100, norm=cr.LogNorm());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(mergers.loc[(mergers.t_merge > 0) & (mergers.t_merge < 1000)].t_form/1000, \n",
    "           np.log10(mergers.loc[(mergers.t_merge > 0) & (mergers.t_merge < 1000)].met/0.017), bins=100, norm=cr.LogNorm());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mergers.loc[(mergers.t_merge > 0) & (mergers.t_merge < 1000)].m1, histtype='step', label='m1')\n",
    "plt.hist(mergers.loc[(mergers.t_merge > 0) & (mergers.t_merge < 1000)].m2, histtype='step', label='m2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers.loc[mergers.met > 0.009].m1.min(), mergers.loc[mergers.met > 0.009].m2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers.loc[mergers.m1 == mergers.m1.max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers.loc[mergers.m2 == mergers.m2.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
